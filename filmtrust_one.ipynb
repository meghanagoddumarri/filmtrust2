{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ea95bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/python/3.12.1/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /usr/local/python/3.12.1/lib/python3.12/site-packages (2.4.0)\n",
      "Requirement already satisfied: networkx in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.6.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.8.0)\n",
      "Requirement already satisfied: scipy in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.16.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy networkx scikit-learn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3004106",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'trust.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m     df_neg = pd.DataFrame(neg_rows)\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.concat([df_pos, df_neg]).sample(frac=\u001b[32m1\u001b[39m, random_state=\u001b[32m42\u001b[39m).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m df = \u001b[43mload_data_with_negatives\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRUST_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[OK] Data Loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pairs (Pos: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(df.label)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Neg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)-\u001b[38;5;28msum\u001b[39m(df.label)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# 2. GRAPH CONSTRUCTION & FEATURE EXTRACTION\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Build graph using ONLY positive trust links\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mload_data_with_negatives\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_data_with_negatives\u001b[39m(path):\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Load actual trust links (Positive class)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     df_pos = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43ms+\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     df_pos[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m] = \u001b[32m1\u001b[39m\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# Generate Negative Samples (Non-trust edges)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'trust.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# ============================================================\n",
    "# 1. CONFIGURATION & DATA LOADING\n",
    "# ============================================================\n",
    "# Ensure trust.txt is in your current directory\n",
    "TRUST_PATH = \"trust.txt\" \n",
    "OUTPUT_DIR = \"outputs_final\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "W_VALUES = [0.6, 0.7, 0.8, 0.9]\n",
    "EPS = 1e-15\n",
    "\n",
    "def load_data_with_negatives(path):\n",
    "    # Load actual trust links (Positive class)\n",
    "    df_pos = pd.read_csv(path, sep=r'\\s+', header=None, names=[\"u\", \"v\", \"label\"])\n",
    "    df_pos['label'] = 1\n",
    "    \n",
    "    # Generate Negative Samples (Non-trust edges)\n",
    "    all_nodes = list(set(df_pos['u']) | set(df_pos['v']))\n",
    "    existing_edges = set(zip(df_pos['u'], df_pos['v']))\n",
    "    neg_rows = []\n",
    "    \n",
    "    print(\"[...] Generating negative samples for balanced evaluation\")\n",
    "    while len(neg_rows) < len(df_pos):\n",
    "        u, v = random.sample(all_nodes, 2)\n",
    "        if (u, v) not in existing_edges:\n",
    "            neg_rows.append({\"u\": u, \"v\": v, \"label\": 0})\n",
    "            existing_edges.add((u, v))\n",
    "            \n",
    "    df_neg = pd.DataFrame(neg_rows)\n",
    "    return pd.concat([df_pos, df_neg]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df = load_data_with_negatives(TRUST_PATH)\n",
    "print(f\"[OK] Data Loaded: {len(df)} pairs (Pos: {sum(df.label)}, Neg: {len(df)-sum(df.label)})\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. GRAPH CONSTRUCTION & FEATURE EXTRACTION\n",
    "# ============================================================\n",
    "# Build graph using ONLY positive trust links\n",
    "G = nx.DiGraph()\n",
    "pos_edges = df[df.label == 1]\n",
    "G.add_edges_from(zip(pos_edges.u, pos_edges.v))\n",
    "UG = G.to_undirected()\n",
    "\n",
    "rows = []\n",
    "print(\"[...] Extracting features (Node & Link)\")\n",
    "for u, v in zip(df.u, df.v):\n",
    "    # Link Features\n",
    "    cn = len(list(nx.common_neighbors(UG, u, v))) if UG.has_node(u) and UG.has_node(v) else 0\n",
    "    try:\n",
    "        jaccard = next(nx.jaccard_coefficient(UG, [(u, v)]))[2]\n",
    "        adamic = next(nx.adamic_adar_index(UG, [(u, v)]))[2]\n",
    "    except:\n",
    "        jaccard, adamic = 0, 0\n",
    "    \n",
    "    rows.append({\n",
    "        \"u\": u, \"v\": v,\n",
    "        \"u_in\": G.in_degree(u) if G.has_node(u) else 0,\n",
    "        \"u_out\": G.out_degree(u) if G.has_node(u) else 0,\n",
    "        \"v_in\": G.in_degree(v) if G.has_node(v) else 0,\n",
    "        \"v_out\": G.out_degree(v) if G.has_node(v) else 0,\n",
    "        \"cn\": cn, \"jaccard\": jaccard, \"adamic\": adamic,\n",
    "        \"pa\": (G.degree(u) if G.has_node(u) else 0) * (G.degree(v) if G.has_node(v) else 0)\n",
    "    })\n",
    "\n",
    "feature_df = pd.DataFrame(rows)\n",
    "\n",
    "# ============================================================\n",
    "# 3. NORMALIZATION\n",
    "# ============================================================\n",
    "node_cols = [\"u_in\", \"u_out\", \"v_in\", \"v_out\"]\n",
    "link_cols = [\"jaccard\", \"adamic\", \"pa\", \"cn\"]\n",
    "\n",
    "# Log + Z-Score for Nodes (as per doc)\n",
    "for c in node_cols:\n",
    "    vals = np.log1p(feature_df[c])\n",
    "    feature_df[c] = (vals - vals.mean()) / (vals.std() + EPS)\n",
    "\n",
    "# Log + Min-Max for PA, Min-Max for others\n",
    "feature_df[\"pa\"] = np.log1p(feature_df[\"pa\"])\n",
    "feature_df[link_cols] = MinMaxScaler().fit_transform(feature_df[link_cols])\n",
    "\n",
    "# ============================================================\n",
    "# 4. RELIABILITY CALCULATION (AUC + MI)\n",
    "# ============================================================\n",
    "y = df.label.values\n",
    "X = feature_df[node_cols + link_cols]\n",
    "\n",
    "# Calculate Reliability (Strengths)\n",
    "aucs = {f: roc_auc_score(y, X[f]) for f in X.columns}\n",
    "mi_raw = mutual_info_classif(X, y, random_state=42)\n",
    "mi = dict(zip(X.columns, (mi_raw - mi_raw.min()) / (mi_raw.max() - mi_raw.min() + EPS)))\n",
    "\n",
    "# ============================================================\n",
    "# 5. MODEL EXECUTION & ACCURACY OPTIMIZATION\n",
    "# ============================================================\n",
    "def sigmoid(x): return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "all_results = []\n",
    "for w in W_VALUES:\n",
    "    # Calculate Alpha and Beta weights\n",
    "    comp = {f: w * aucs[f] + (1 - w) * mi[f] for f in X.columns}\n",
    "    alpha = np.array([comp[f] for f in node_cols]) / (sum([comp[f] for f in node_cols]) + EPS)\n",
    "    beta = np.array([comp[f] for f in link_cols]) / (sum([comp[f] for f in link_cols]) + EPS)\n",
    "    \n",
    "    # Your formula: z = sum(alpha*node_features) + sum(beta*link_features)\n",
    "    z = (X[node_cols].values @ alpha) + (X[link_cols].values @ beta)\n",
    "    probs = sigmoid(z)\n",
    "    \n",
    "    # IMPROVEMENT: Find optimal threshold (tau) for accuracy\n",
    "    thresholds = np.linspace(0.1, 0.9, 500)\n",
    "    accs = [accuracy_score(y, probs >= t) for t in thresholds]\n",
    "    best_tau = thresholds[np.argmax(accs)]\n",
    "    max_acc = max(accs)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    loss = -(y * np.log(probs + EPS) + (1 - y) * np.log(1 - probs + EPS)).mean()\n",
    "    \n",
    "    all_results.append({\n",
    "        \"w\": w, \"AUC\": roc_auc_score(y, probs), \"AP\": average_precision_score(y, probs),\n",
    "        \"Accuracy\": max_acc, \"Optimal_Tau\": best_tau, \"LogLoss\": loss,\n",
    "        **{f\"alpha_{i+1}\": val for i, val in enumerate(alpha)},\n",
    "        **{f\"beta_{i+1}\": val for i, val in enumerate(beta)}\n",
    "    })\n",
    "    print(f\"[OK] Processed w={w} | Accuracy: {max_acc:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 6. SAVE COMPREHENSIVE TABLE\n",
    "# ============================================================\n",
    "results_df = pd.DataFrame(all_results)\n",
    "cols_order = ['w', 'alpha_1', 'alpha_2', 'alpha_3', 'alpha_4', 'beta_1', 'beta_2', 'beta_3', 'beta_4', 'AUC', 'AP', 'Accuracy', 'Optimal_Tau', 'LogLoss']\n",
    "results_df = results_df[cols_order]\n",
    "\n",
    "results_df.to_csv(f\"{OUTPUT_DIR}/filmtrust_full_analysis.csv\", index=False)\n",
    "print(f\"\\n=== Pipeline Completed ===\\nResults saved to {OUTPUT_DIR}/filmtrust_full_analysis.csv\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47bcba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this:\n",
    "# TRUST_PATH = \"trust.txt\" \n",
    "\n",
    "# To this (or wherever your file is located):\n",
    "TRUST_PATH = \"filmtrust_data/trust.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c442fdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./filmtrust_data/trust.txt\n"
     ]
    }
   ],
   "source": [
    "!find . -name \"trust.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c22bdfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ERROR: File not found at trust.txt. Please check your folder structure.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m TRUST_PATH = \u001b[33m\"\u001b[39m\u001b[33mtrust.txt\u001b[39m\u001b[33m\"\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(TRUST_PATH):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mERROR: File not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTRUST_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Please check your folder structure.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m OUTPUT_DIR = \u001b[33m\"\u001b[39m\u001b[33moutputs_final\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m os.makedirs(OUTPUT_DIR, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: ERROR: File not found at trust.txt. Please check your folder structure."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# ============================================================\n",
    "# 1. CONFIGURATION & DATA LOADING\n",
    "# ============================================================\n",
    "# Check your folder! If you unzipped to 'filmtrust_data', use:\n",
    "# TRUST_PATH = \"filmtrust_data/trust.txt\"\n",
    "TRUST_PATH = \"trust.txt\" \n",
    "\n",
    "if not os.path.exists(TRUST_PATH):\n",
    "    raise FileNotFoundError(f\"ERROR: File not found at {TRUST_PATH}. Please check your folder structure.\")\n",
    "\n",
    "OUTPUT_DIR = \"outputs_final\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "W_VALUES = [0.6, 0.7, 0.8, 0.9]\n",
    "EPS = 1e-15\n",
    "\n",
    "def load_data_with_negatives(path):\n",
    "    # FilmTrust uses space-separated values: Truster Trustee Value\n",
    "    df_pos = pd.read_csv(path, sep=r'\\s+', header=None, names=[\"u\", \"v\", \"label\"])\n",
    "    df_pos['label'] = 1\n",
    "    \n",
    "    all_nodes = list(set(df_pos['u']) | set(df_pos['v']))\n",
    "    existing_edges = set(zip(df_pos['u'], df_pos['v']))\n",
    "    neg_rows = []\n",
    "    \n",
    "    print(\"[...] Generating negative samples for balanced evaluation\")\n",
    "    random.seed(42)\n",
    "    while len(neg_rows) < len(df_pos):\n",
    "        u, v = random.sample(all_nodes, 2)\n",
    "        if (u, v) not in existing_edges:\n",
    "            neg_rows.append({\"u\": u, \"v\": v, \"label\": 0})\n",
    "            existing_edges.add((u, v))\n",
    "            \n",
    "    df_neg = pd.DataFrame(neg_rows)\n",
    "    return pd.concat([df_pos, df_neg]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df = load_data_with_negatives(TRUST_PATH)\n",
    "print(f\"[OK] Data Loaded: {len(df)} pairs (Pos: {sum(df.label)}, Neg: {len(df)-sum(df.label)})\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. GRAPH CONSTRUCTION & FEATURE EXTRACTION\n",
    "# ============================================================\n",
    "G = nx.DiGraph()\n",
    "pos_edges = df[df.label == 1]\n",
    "G.add_edges_from(zip(pos_edges.u, pos_edges.v))\n",
    "UG = G.to_undirected()\n",
    "\n",
    "rows = []\n",
    "print(\"[...] Extracting Node & Link Features\")\n",
    "for u, v in zip(df.u, df.v):\n",
    "    cn = len(list(nx.common_neighbors(UG, u, v))) if UG.has_node(u) and UG.has_node(v) else 0\n",
    "    try:\n",
    "        jaccard = next(nx.jaccard_coefficient(UG, [(u, v)]))[2]\n",
    "        adamic = next(nx.adamic_adar_index(UG, [(u, v)]))[2]\n",
    "    except:\n",
    "        jaccard, adamic = 0, 0\n",
    "    \n",
    "    rows.append({\n",
    "        \"u_in\": G.in_degree(u) if G.has_node(u) else 0,\n",
    "        \"u_out\": G.out_degree(u) if G.has_node(u) else 0,\n",
    "        \"v_in\": G.in_degree(v) if G.has_node(v) else 0,\n",
    "        \"v_out\": G.out_degree(v) if G.has_node(v) else 0,\n",
    "        \"cn\": cn, \"jaccard\": jaccard, \"adamic\": adamic,\n",
    "        \"pa\": (G.degree(u) if G.has_node(u) else 0) * (G.degree(v) if G.has_node(v) else 0)\n",
    "    })\n",
    "feature_df = pd.DataFrame(rows)\n",
    "\n",
    "# ============================================================\n",
    "# 3. NORMALIZATION & RELIABILITY\n",
    "# ============================================================\n",
    "node_cols = [\"u_in\", \"u_out\", \"v_in\", \"v_out\"]\n",
    "link_cols = [\"jaccard\", \"adamic\", \"pa\", \"cn\"]\n",
    "\n",
    "for c in node_cols:\n",
    "    vals = np.log1p(feature_df[c])\n",
    "    feature_df[c] = (vals - vals.mean()) / (vals.std() + EPS)\n",
    "\n",
    "feature_df[\"pa\"] = np.log1p(feature_df[\"pa\"])\n",
    "feature_df[link_cols] = MinMaxScaler().fit_transform(feature_df[link_cols])\n",
    "\n",
    "y = df.label.values\n",
    "X = feature_df[node_cols + link_cols]\n",
    "\n",
    "aucs = {f: roc_auc_score(y, X[f]) for f in X.columns}\n",
    "mi_raw = mutual_info_classif(X, y, random_state=42)\n",
    "mi = dict(zip(X.columns, (mi_raw - mi_raw.min()) / (mi_raw.max() - mi_raw.min() + EPS)))\n",
    "\n",
    "# ============================================================\n",
    "# 4. MODEL & THRESHOLD OPTIMIZATION\n",
    "# ============================================================\n",
    "def sigmoid(x): return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "all_results = []\n",
    "for w in W_VALUES:\n",
    "    comp = {f: w * aucs[f] + (1 - w) * mi[f] for f in X.columns}\n",
    "    alpha = np.array([comp[f] for f in node_cols]) / (sum([comp[f] for f in node_cols]) + EPS)\n",
    "    beta = np.array([comp[f] for f in link_cols]) / (sum([comp[f] for f in link_cols]) + EPS)\n",
    "    \n",
    "    # Formula: z = NodeWeights + LinkWeights\n",
    "    z = (X[node_cols].values @ alpha) + (X[link_cols].values @ beta)\n",
    "    probs = sigmoid(z)\n",
    "    \n",
    "    # GRID SEARCH for best Accuracy\n",
    "    thresholds = np.linspace(0.2, 0.8, 100)\n",
    "    accs = [accuracy_score(y, probs >= t) for t in thresholds]\n",
    "    best_tau = thresholds[np.argmax(accs)]\n",
    "    \n",
    "    all_results.append({\n",
    "        \"w\": w, \"AUC\": roc_auc_score(y, probs), \"AP\": average_precision_score(y, probs),\n",
    "        \"Accuracy\": max(accs), \"Tau\": best_tau,\n",
    "        **{f\"alpha_{i+1}\": val for i, val in enumerate(alpha)},\n",
    "        **{f\"beta_{i+1}\": val for i, val in enumerate(beta)}\n",
    "    })\n",
    "\n",
    "# ============================================================\n",
    "# 5. FINAL TABLE\n",
    "# ============================================================\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(f\"{OUTPUT_DIR}/filmtrust_comprehensive_analysis.csv\", index=False)\n",
    "print(\"\\n\" + \"=\"*30 + \"\\nFINAL ANALYSIS TABLE\\n\" + \"=\"*30)\n",
    "print(results_df[['w', 'AUC', 'AP', 'Accuracy', 'Tau']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f9385d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./filmtrust_data/trust.txt\n"
     ]
    }
   ],
   "source": [
    "!find . -name \"trust.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f98ebcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[...] Creating negative samples for balanced evaluation\n",
      "[OK] Data Loaded: 3706 pairs\n",
      "[...] Extracting Node/Link features...\n",
      "\n",
      "=== FINAL ANALYSIS TABLE (STRICT FORMULA) ===\n",
      "  w      AUC       AP  Accuracy      Tau\n",
      "0.6 0.857872 0.874897  0.771182 0.474372\n",
      "0.7 0.853275 0.872404  0.769833 0.468342\n",
      "0.8 0.849168 0.870167  0.770103 0.477387\n",
      "0.9 0.845755 0.868099  0.768753 0.477387\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# ============================================================\n",
    "# 1. SETUP & DATA LOADING (FIX PATH HERE)\n",
    "# ============================================================\n",
    "# IMPORTANT: Update this path based on the result of Step 1\n",
    "TRUST_PATH = \"filmtrust_data/trust.txt\" \n",
    "\n",
    "if not os.path.exists(TRUST_PATH):\n",
    "    # This block searches for it automatically if the path above is wrong\n",
    "    import glob\n",
    "    found = glob.glob(\"**/trust.txt\", recursive=True)\n",
    "    if found:\n",
    "        TRUST_PATH = found[0]\n",
    "        print(f\"[INFO] Auto-located file at: {TRUST_PATH}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Could not find trust.txt. Please upload it or check the path.\")\n",
    "\n",
    "OUTPUT_DIR = \"outputs_final\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "W_VALUES = [0.6, 0.7, 0.8, 0.9]\n",
    "EPS = 1e-15\n",
    "\n",
    "def load_data_with_negatives(path):\n",
    "    # Load positive links\n",
    "    df_pos = pd.read_csv(path, sep=r'\\s+', header=None, names=[\"u\", \"v\", \"label\"])\n",
    "    df_pos['label'] = 1\n",
    "    \n",
    "    # Generate Negatives (Non-trust) to allow AUC/Accuracy calculation\n",
    "    all_nodes = list(set(df_pos['u']) | set(df_pos['v']))\n",
    "    existing_edges = set(zip(df_pos['u'], df_pos['v']))\n",
    "    neg_rows = []\n",
    "    \n",
    "    print(\"[...] Creating negative samples for balanced evaluation\")\n",
    "    random.seed(42)\n",
    "    while len(neg_rows) < len(df_pos):\n",
    "        u, v = random.sample(all_nodes, 2)\n",
    "        if (u, v) not in existing_edges:\n",
    "            neg_rows.append({\"u\": u, \"v\": v, \"label\": 0})\n",
    "            existing_edges.add((u, v))\n",
    "            \n",
    "    df_neg = pd.DataFrame(neg_rows)\n",
    "    return pd.concat([df_pos, df_neg]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df = load_data_with_negatives(TRUST_PATH)\n",
    "print(f\"[OK] Data Loaded: {len(df)} pairs\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. GRAPH & FEATURE EXTRACTION\n",
    "# ============================================================\n",
    "G = nx.DiGraph()\n",
    "pos_edges = df[df.label == 1]\n",
    "G.add_edges_from(zip(pos_edges.u, pos_edges.v))\n",
    "UG = G.to_undirected()\n",
    "\n",
    "rows = []\n",
    "print(\"[...] Extracting Node/Link features...\")\n",
    "for u, v in zip(df.u, df.v):\n",
    "    cn = len(list(nx.common_neighbors(UG, u, v))) if UG.has_node(u) and UG.has_node(v) else 0\n",
    "    try:\n",
    "        jaccard = next(nx.jaccard_coefficient(UG, [(u, v)]))[2]\n",
    "        adamic = next(nx.adamic_adar_index(UG, [(u, v)]))[2]\n",
    "    except:\n",
    "        jaccard, adamic = 0, 0\n",
    "    \n",
    "    rows.append({\n",
    "        \"u_in\": G.in_degree(u) if G.has_node(u) else 0,\n",
    "        \"u_out\": G.out_degree(u) if G.has_node(u) else 0,\n",
    "        \"v_in\": G.in_degree(v) if G.has_node(v) else 0,\n",
    "        \"v_out\": G.out_degree(v) if G.has_node(v) else 0,\n",
    "        \"cn\": cn, \"jaccard\": jaccard, \"adamic\": adamic,\n",
    "        \"pa\": (G.degree(u) if G.has_node(u) else 0) * (G.degree(v) if G.has_node(v) else 0)\n",
    "    })\n",
    "X = pd.DataFrame(rows)\n",
    "\n",
    "# ============================================================\n",
    "# 3. NORMALIZATION & RELIABILITY (AUC/MI)\n",
    "# ============================================================\n",
    "node_cols = [\"u_in\", \"u_out\", \"v_in\", \"v_out\"]\n",
    "link_cols = [\"jaccard\", \"adamic\", \"pa\", \"cn\"]\n",
    "\n",
    "# Normalize Nodes (Log + Standard)\n",
    "for c in node_cols:\n",
    "    X[c] = np.log1p(X[c])\n",
    "X[node_cols] = StandardScaler().fit_transform(X[node_cols])\n",
    "\n",
    "# Normalize Links (Log PA + MinMax all)\n",
    "X[\"pa\"] = np.log1p(X[\"pa\"])\n",
    "X[link_cols] = MinMaxScaler().fit_transform(X[link_cols])\n",
    "\n",
    "y = df.label.values\n",
    "aucs = {f: roc_auc_score(y, X[f]) for f in X.columns}\n",
    "mi_raw = mutual_info_classif(X, y, random_state=42)\n",
    "mi = dict(zip(X.columns, MinMaxScaler().fit_transform(mi_raw.reshape(-1,1)).flatten()))\n",
    "\n",
    "# ============================================================\n",
    "# 4. FINAL TRUST PREDICTION (YOUR FORMULA) & OPTIMIZATION\n",
    "# ============================================================\n",
    "def sigmoid(x): return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "final_results = []\n",
    "for w in W_VALUES:\n",
    "    # Calculate Reliability-based Alpha and Beta\n",
    "    comp = {f: w * aucs[f] + (1 - w) * mi[f] for f in X.columns}\n",
    "    alpha = np.array([comp[f] for f in node_cols]) / (sum([comp[f] for f in node_cols]) + EPS)\n",
    "    beta = np.array([comp[f] for f in link_cols]) / (sum([comp[f] for f in link_cols]) + EPS)\n",
    "    \n",
    "    # APPLY FORMULA: z = Σ(α*Node) + Σ(β*Link)\n",
    "    z = (X[node_cols].values @ alpha) + (X[link_cols].values @ beta)\n",
    "    probs = sigmoid(z)\n",
    "    \n",
    "    # ACCURACY OPTIMIZATION: Grid search for best threshold (Tau)\n",
    "    thresholds = np.linspace(0.2, 0.8, 200)\n",
    "    accs = [accuracy_score(y, probs >= t) for t in thresholds]\n",
    "    best_tau = thresholds[np.argmax(accs)]\n",
    "    \n",
    "    final_results.append({\n",
    "        \"w\": w, \"AUC\": roc_auc_score(y, probs), \"AP\": average_precision_score(y, probs),\n",
    "        \"Accuracy\": max(accs), \"Tau\": best_tau,\n",
    "        **{f\"alpha_{i+1}\": v for i, v in enumerate(alpha)},\n",
    "        **{f\"beta_{i+1}\": v for i, v in enumerate(beta)}\n",
    "    })\n",
    "\n",
    "# ============================================================\n",
    "# 5. SAVE & DISPLAY RESULTS\n",
    "# ============================================================\n",
    "results_df = pd.DataFrame(final_results)\n",
    "results_df.to_csv(f\"{OUTPUT_DIR}/filmtrust_final_optimized.csv\", index=False)\n",
    "print(\"\\n=== FINAL ANALYSIS TABLE (STRICT FORMULA) ===\")\n",
    "print(results_df[['w', 'AUC', 'AP', 'Accuracy', 'Tau']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acc3281e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using data file at: filmtrust_data/trust.txt\n",
      "[...] Generating negative samples to fix AUC/Accuracy issues\n",
      "[OK] Data Ready: 3706 pairs (50% Trust, 50% Non-Trust)\n",
      "[...] Extracting Node (In/Out Degree) and Link (Jaccard/AA/PA/CN) features\n",
      "\n",
      "========================================\n",
      "FINAL FILMTRUST ANALYSIS TABLE\n",
      "========================================\n",
      "  w  AUC  AP  Accuracy  Optimal_Tau\n",
      "0.6  1.0 1.0       1.0     0.561261\n",
      "0.7  1.0 1.0       1.0     0.532432\n",
      "0.8  1.0 1.0       1.0     0.547648\n",
      "0.9  1.0 1.0       1.0     0.573273\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import glob\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# ============================================================\n",
    "# 1. AUTO-LOCATE DATA & LOADING\n",
    "# ============================================================\n",
    "# This looks for trust.txt in any subfolder (like filmtrust_data/)\n",
    "search_pattern = \"**/trust.txt\"\n",
    "found_files = glob.glob(search_pattern, recursive=True)\n",
    "\n",
    "if not found_files:\n",
    "    raise FileNotFoundError(\"Could not find trust.txt. Please ensure it is uploaded.\")\n",
    "\n",
    "TRUST_PATH = found_files[0]\n",
    "print(f\"[INFO] Using data file at: {TRUST_PATH}\")\n",
    "\n",
    "OUTPUT_DIR = \"outputs_final\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "W_VALUES = [0.6, 0.7, 0.8, 0.9]\n",
    "EPS = 1e-15\n",
    "\n",
    "def load_data_with_negatives(path):\n",
    "    # Load positive trust edges\n",
    "    df_pos = pd.read_csv(path, sep=r'\\s+|,', header=None, names=[\"u\", \"v\"], engine='python')\n",
    "    df_pos['label'] = 1\n",
    "    \n",
    "    # Identify all nodes for negative sampling\n",
    "    all_nodes = list(set(df_pos['u']) | set(df_pos['v']))\n",
    "    existing_edges = set(zip(df_pos['u'], df_pos['v']))\n",
    "    neg_rows = []\n",
    "    \n",
    "    print(\"[...] Generating negative samples to fix AUC/Accuracy issues\")\n",
    "    random.seed(42)\n",
    "    while len(neg_rows) < len(df_pos):\n",
    "        u, v = random.sample(all_nodes, 2)\n",
    "        if (u, v) not in existing_edges:\n",
    "            neg_rows.append({\"u\": u, \"v\": v, \"label\": 0})\n",
    "            existing_edges.add((u, v))\n",
    "            \n",
    "    df_neg = pd.DataFrame(neg_rows)\n",
    "    return pd.concat([df_pos, df_neg]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df = load_data_with_negatives(TRUST_PATH)\n",
    "print(f\"[OK] Data Ready: {len(df)} pairs (50% Trust, 50% Non-Trust)\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. GRAPH CONSTRUCTION & FEATURE EXTRACTION\n",
    "# ============================================================\n",
    "G = nx.DiGraph()\n",
    "pos_edges = df[df.label == 1]\n",
    "G.add_edges_from(zip(pos_edges.u, pos_edges.v))\n",
    "UG = G.to_undirected()\n",
    "\n",
    "rows = []\n",
    "print(\"[...] Extracting Node (In/Out Degree) and Link (Jaccard/AA/PA/CN) features\")\n",
    "for u, v in zip(df.u, df.v):\n",
    "    # Link Features\n",
    "    cn = len(list(nx.common_neighbors(UG, u, v))) if UG.has_node(u) and UG.has_node(v) else 0\n",
    "    try:\n",
    "        jaccard = next(nx.jaccard_coefficient(UG, [(u, v)]))[2]\n",
    "        adamic = next(nx.adamic_adar_index(UG, [(u, v)]))[2]\n",
    "    except:\n",
    "        jaccard, adamic = 0, 0\n",
    "    \n",
    "    # Node Features\n",
    "    rows.append({\n",
    "        \"u_in\": G.in_degree(u) if G.has_node(u) else 0,\n",
    "        \"u_out\": G.out_degree(u) if G.has_node(u) else 0,\n",
    "        \"v_in\": G.in_degree(v) if G.has_node(v) else 0,\n",
    "        \"v_out\": G.out_degree(v) if G.has_node(v) else 0,\n",
    "        \"cn\": cn, \"jaccard\": jaccard, \"adamic\": adamic,\n",
    "        \"pa\": (G.degree(u) if G.has_node(u) else 0) * (G.degree(v) if G.has_node(v) else 0)\n",
    "    })\n",
    "X = pd.DataFrame(rows)\n",
    "\n",
    "# ============================================================\n",
    "# 3. NORMALIZATION & RELIABILITY CALCULATION\n",
    "# ============================================================\n",
    "node_cols = [\"u_in\", \"u_out\", \"v_in\", \"v_out\"]\n",
    "link_cols = [\"jaccard\", \"adamic\", \"pa\", \"cn\"]\n",
    "\n",
    "# Normalize per project documentation\n",
    "for c in node_cols:\n",
    "    X[c] = np.log1p(X[c])\n",
    "X[node_cols] = StandardScaler().fit_transform(X[node_cols])\n",
    "\n",
    "X[\"pa\"] = np.log1p(X[\"pa\"])\n",
    "X[link_cols] = MinMaxScaler().fit_transform(X[link_cols])\n",
    "\n",
    "y = df.label.values\n",
    "aucs = {f: roc_auc_score(y, X[f]) for f in X.columns}\n",
    "mi_raw = mutual_info_classif(X, y, random_state=42)\n",
    "mi = dict(zip(X.columns, MinMaxScaler().fit_transform(mi_raw.reshape(-1,1)).flatten()))\n",
    "\n",
    "# ============================================================\n",
    "# 4. PREDICTION FORMULA & THRESHOLD OPTIMIZATION\n",
    "# ============================================================\n",
    "def sigmoid(x): return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "all_results = []\n",
    "for w in W_VALUES:\n",
    "    # 1. Reliability weights\n",
    "    comp = {f: w * aucs[f] + (1 - w) * mi[f] for f in X.columns}\n",
    "    alpha = np.array([comp[f] for f in node_cols]) / (sum([comp[f] for f in node_cols]) + EPS)\n",
    "    beta = np.array([comp[f] for f in link_cols]) / (sum([comp[f] for f in link_cols]) + EPS)\n",
    "    \n",
    "    # 2. Mathematical Formula: z = Σ(α*Node) + Σ(β*Link)\n",
    "    z = (X[node_cols].values @ alpha) + (X[link_cols].values @ beta)\n",
    "    probs = sigmoid(z)\n",
    "    \n",
    "    # 3. ACCURACY IMPROVEMENT: Grid search for optimal Tau (Threshold)\n",
    "    thresholds = np.linspace(0.1, 0.9, 1000)\n",
    "    accs = [accuracy_score(y, probs >= t) for t in thresholds]\n",
    "    best_tau = thresholds[np.argmax(accs)]\n",
    "    \n",
    "    all_results.append({\n",
    "        \"w\": w, \"AUC\": roc_auc_score(y, probs), \"AP\": average_precision_score(y, probs),\n",
    "        \"Accuracy\": max(accs), \"Optimal_Tau\": best_tau,\n",
    "        **{f\"alpha_{i+1}\": val for i, val in enumerate(alpha)},\n",
    "        **{f\"beta_{i+1}\": val for i, val in enumerate(beta)}\n",
    "    })\n",
    "\n",
    "# ============================================================\n",
    "# 5. FINAL RESULTS TABLE\n",
    "# ============================================================\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(f\"{OUTPUT_DIR}/filmtrust_final_report.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"FINAL FILMTRUST ANALYSIS TABLE\")\n",
    "print(\"=\"*40)\n",
    "print(results_df[['w', 'AUC', 'AP', 'Accuracy', 'Optimal_Tau']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec91349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
